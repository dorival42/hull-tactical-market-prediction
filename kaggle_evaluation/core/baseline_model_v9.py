"""
Pipeline complet pour Hull Tactical Market Prediction
VERSION FINALE OPTIMIS√âE avec Preprocessor V2
Baseline avec XGBoost et feature engineering avanc√©
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Import du preprocessor V2 optimis√©
from preprocessor_v2 import HullPreprocessorV2
# LIGNE 17 - Remplacer:
from preprocessor_v2 import HullPreprocessorV2

# Par:
from preprocessor_simple import HullPreprocessorSimple as HullPreprocessorV2
# ==============================================================================
# 1. CONFIGURATION
# ==============================================================================

CONFIG = {
    'random_state': 42,
    'n_splits': 5,
    'test_size': 0.2,
    'model_path': 'xgb_model.pkl',
    'preprocessor_path': 'preprocessor.pkl',
    'feature_importance_path': 'feature_importance.csv',
    
    # Hyperparam√®tres XGBoost (optimis√©s pour Sharpe Ratio)
    'xgb_params': {
        'n_estimators': 700,
        'max_depth': 6,
        'learning_rate': 0.008,
        'subsample': 0.85,
        'colsample_bytree': 0.85,
        'min_child_weight': 3,
        'gamma': 0.1,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'random_state': 42,
        'n_jobs': -1,
        'tree_method': 'hist'
    }
}

# ==============================================================================
# 2. CALCUL DU SHARPE RATIO
# ==============================================================================

def calculate_sharpe_ratio(returns, risk_free_rate=0):
    """
    Calculer le Sharpe Ratio annualis√©
    
    Args:
        returns: pr√©dictions ou rendements (array-like)
        risk_free_rate: taux sans risque (d√©j√† pris en compte dans excess returns)
    
    Returns:
        Sharpe Ratio annualis√© (float)
    """
    # Convertir en array numpy si n√©cessaire
    returns = np.asarray(returns).flatten()
    
    mean_return = float(np.mean(returns))
    std_return = float(np.std(returns))
    
    if std_return == 0 or np.isnan(std_return):
        return 0.0
    
    sharpe = (mean_return - risk_free_rate) / std_return
    
    # Annualiser (252 jours de trading)
    sharpe_annualized = float(sharpe * np.sqrt(252))
    
    return sharpe_annualized


def calculate_sortino_ratio(returns, risk_free_rate=0):
    """Calculer le Sortino Ratio (comme Sharpe mais uniquement downside risk)"""
    returns = np.asarray(returns).flatten()
    
    mean_return = float(np.mean(returns))
    downside_returns = returns[returns < 0]
    
    if len(downside_returns) == 0:
        return 0.0
    
    downside_std = float(np.std(downside_returns))
    
    if downside_std == 0 or np.isnan(downside_std):
        return 0.0
    
    sortino = (mean_return - risk_free_rate) / downside_std
    sortino_annualized = float(sortino * np.sqrt(252))
    
    return sortino_annualized


def calculate_max_drawdown(returns):
    """Calculer le Maximum Drawdown"""
    if isinstance(returns, pd.Series):
        returns_series = returns
    else:
        returns_series = pd.Series(returns)
    
    cumulative = (1 + returns_series).cumprod()
    running_max = cumulative.cummax()
    drawdown = (cumulative - running_max) / running_max
    
    return float(drawdown.min())


def calculate_metrics(y_true, y_pred):
    """Calculer toutes les m√©triques de performance"""
    
    # S'assurer que ce sont des arrays numpy
    y_true = np.asarray(y_true).flatten()
    y_pred = np.asarray(y_pred).flatten()
    
    # Erreurs de pr√©diction
    mse = float(mean_squared_error(y_true, y_pred))
    mae = float(mean_absolute_error(y_true, y_pred))
    
    # Sharpe Ratio (m√©trique principale) - utiliser y_pred uniquement
    sharpe = calculate_sharpe_ratio(y_pred)
    
    # Sortino Ratio
    sortino = calculate_sortino_ratio(y_pred)
    
    # Hit rate (% de fois o√π on pr√©dit le bon signe)
    hit_rate = float(np.mean(np.sign(y_pred) == np.sign(y_true)))
    
    # Maximum Drawdown
    max_dd = calculate_max_drawdown(y_pred)
    
    # Corr√©lation
    if len(y_pred) > 1:
        corr_matrix = np.corrcoef(y_pred, y_true)
        correlation = float(corr_matrix[0, 1]) if not np.isnan(corr_matrix[0, 1]) else 0.0
    else:
        correlation = 0.0
    
    # Volatilit√©
    volatility = float(np.std(y_pred))
    
    return {
        'MSE': mse,
        'RMSE': float(np.sqrt(mse)),
        'MAE': mae,
        'Sharpe': sharpe,
        'Sortino': sortino,
        'Hit_Rate': hit_rate,
        'Max_Drawdown': max_dd,
        'Correlation': correlation,
        'Volatility': volatility
    }

# ==============================================================================
# 3. TRAINING PIPELINE
# ==============================================================================

def train_baseline_model():
    """Pipeline complet d'entra√Ænement"""
    
    print("=" * 80)
    print("HULL TACTICAL - TRAINING PIPELINE V2")
    print("=" * 80)
    print(f"Configuration:")
    print(f"  - Preprocessor: HullPreprocessorV2")
    print(f"  - Model: XGBoost")
    print(f"  - Validation split: {CONFIG['test_size']*100}%")
    print(f"  - Random state: {CONFIG['random_state']}")
    
    # 1. Charger les donn√©es
    print("\n" + "=" * 80)
    print("1. CHARGEMENT DES DONN√âES")
    print("=" * 80)
    
    if not Path('train.csv').exists():
        raise FileNotFoundError("train.csv not found. Please download the data first.")
    
    train = pd.read_csv('train.csv')
    print(f"‚úì Train shape: {train.shape}")
    print(f"‚úì P√©riode: date_id {train['date_id'].min()} √† {train['date_id'].max()}")
    print(f"‚úì Colonnes: {len(train.columns)}")
    
    # V√©rifier la target
    target = 'market_forward_excess_returns'
    if target not in train.columns:
        raise ValueError(f"Target column '{target}' not found in train.csv")
    
    y = train[target].values
    print(f"\n‚úì Target: {target}")
    print(f"  - Mean: {y.mean():.6f}")
    print(f"  - Std: {y.std():.6f}")
    print(f"  - Min: {y.min():.6f}")
    print(f"  - Max: {y.max():.6f}")
    
    # 2. Preprocessing avec V2
    print("\n" + "=" * 80)
    print("2. PREPROCESSING (HullPreprocessorV2)")
    print("=" * 80)
    
    preprocessor = HullPreprocessorV2(verbose=True)
    X = preprocessor.fit_transform(train)
    
    print(f"\n‚úì Features shape: {X.shape}")
    print(f"‚úì Nombre de features: {len(preprocessor.feature_names)}")
    
    # R√©sum√© des features
    summary = preprocessor.get_feature_summary()
    print(f"\nR√©sum√© des features:")
    print(f"  - Original: {summary['original_features']}")
    print(f"  - Lagged: {summary['lagged_features']}")
    print(f"  - Created: {summary['created_features']}")
    print(f"  - Total: {summary['total_features']}")
    
    # 3. Train/Validation split (time-based)
    print("\n" + "=" * 80)
    print("3. TRAIN/VALIDATION SPLIT")
    print("=" * 80)
    
    split_idx = int(len(X) * (1 - CONFIG['test_size']))
    
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    print(f"‚úì Train set: {X_train.shape}")
    print(f"  - date_id range: {train['date_id'].iloc[:split_idx].min()} √† {train['date_id'].iloc[:split_idx].max()}")
    print(f"  - Target mean: {y_train.mean():.6f}")
    
    print(f"\n‚úì Validation set: {X_val.shape}")
    print(f"  - date_id range: {train['date_id'].iloc[split_idx:].min()} √† {train['date_id'].iloc[split_idx:].max()}")
    print(f"  - Target mean: {y_val.mean():.6f}")
    
    # 4. Entra√Æner le mod√®le
    print("\n" + "=" * 80)
    print("4. ENTRA√éNEMENT DU MOD√àLE XGBOOST")
    print("=" * 80)
    print(f"Hyperparam√®tres:")
    for key, value in CONFIG['xgb_params'].items():
        print(f"  {key:20s}: {value}")
    
    print(f"\n‚è≥ Training en cours...")
    print(f"‚ö†Ô∏è  Cela peut prendre 5-10 minutes...\n")
    
    # Cr√©er le mod√®le SANS eval_metric et early_stopping dans le constructeur
    model = xgb.XGBRegressor(**CONFIG['xgb_params'])
    
    # Fit avec eval_set mais SANS early_stopping pour forcer le training complet
    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        verbose=100  # Afficher tous les 100 iterations
    )
    
    print(f"\n‚úì Training termin√©")
    print(f"‚úì Total iterations: {model.n_estimators}")
    
    # 5. √âvaluation
    print("\n" + "=" * 80)
    print("5. √âVALUATION DES PERFORMANCES")
    print("=" * 80)
    
    # Pr√©dictions
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    
    # M√©triques
    train_metrics = calculate_metrics(y_train, y_train_pred)
    val_metrics = calculate_metrics(y_val, y_val_pred)
    
    print("\nüìä TRAIN METRICS:")
    for metric, value in train_metrics.items():
        # Convertir en scalaire si c'est un array
        if isinstance(value, np.ndarray):
            value = float(value)
        print(f"  {metric:15s}: {value:.6f}")
    
    print("\nüìä VALIDATION METRICS:")
    for metric, value in val_metrics.items():
        # Convertir en scalaire si c'est un array
        if isinstance(value, np.ndarray):
            value = float(value)
        print(f"  {metric:15s}: {value:.6f}")
    
    # √âvaluation du Sharpe Ratio
    sharpe_val = val_metrics['Sharpe']
    print("\n" + "=" * 80)
    print("√âVALUATION DU SHARPE RATIO")
    print("=" * 80)
    
    if sharpe_val > 1.0:
        print(f"üèÜ EXCELLENT ! Sharpe = {sharpe_val:.4f}")
        print("   Vous √™tes dans le top 10% !")
    elif sharpe_val > 0.7:
        print(f"üîµ TR√àS BON ! Sharpe = {sharpe_val:.4f}")
        print("   Vous √™tes tr√®s comp√©titif !")
    elif sharpe_val > 0.5:
        print(f"üü¢ BON ! Sharpe = {sharpe_val:.4f}")
        print("   Vous √™tes comp√©titif, continuez √† optimiser !")
    elif sharpe_val > 0.3:
        print(f"üü° ACCEPTABLE. Sharpe = {sharpe_val:.4f}")
        print("   Baseline fonctionnel, optimisation recommand√©e.")
    else:
        print(f"‚ö†Ô∏è  FAIBLE. Sharpe = {sharpe_val:.4f}")
        print("   V√©rifiez les features et les hyperparam√®tres.")
    
    # 6. Feature importance
    print("\n" + "=" * 80)
    print("6. FEATURE IMPORTANCE")
    print("=" * 80)
    
    feature_importance = pd.DataFrame({
        'feature': preprocessor.feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüîù Top 20 Features importantes:")
    print(feature_importance.head(20).to_string(index=False))
    
    # V√©rifier que les lagged features sont importantes
    top_10_features = feature_importance.head(10)['feature'].tolist()
    lagged_in_top10 = sum(1 for f in top_10_features if 'lagged' in f or 'target' in f)
    
    print(f"\nüìä Features lagged/target dans le top 10: {lagged_in_top10}/10")
    if lagged_in_top10 >= 3:
        print("‚úÖ Bon ! Les lagged features sont bien utilis√©es.")
    else:
        print("‚ö†Ô∏è  Warning: Peu de lagged features dans le top 10.")
    
    # Sauvegarder feature importance
    feature_importance.to_csv(CONFIG['feature_importance_path'], index=False)
    print(f"\n‚úì Feature importance sauvegard√©e: {CONFIG['feature_importance_path']}")
    
    # 7. Analyse par r√©gime de march√©
    print("\n" + "=" * 80)
    print("7. ANALYSE PAR R√âGIME DE MARCH√â")
    print("=" * 80)
    
    # D√©finir les r√©gimes bas√©s sur les returns
    regimes_val = pd.cut(y_val, 
                         bins=[-np.inf, -0.005, 0.005, np.inf],
                         labels=['Bear', 'Neutral', 'Bull'])
    
    print("\nüìä Performance par r√©gime (Validation):\n")
    
    for regime in ['Bear', 'Neutral', 'Bull']:
        mask = regimes_val == regime
        if mask.sum() == 0:
            continue
        
        pred_regime = y_val_pred[mask]
        actual_regime = y_val[mask]
        
        sharpe_regime = calculate_sharpe_ratio(pred_regime)
        hit_rate_regime = np.mean(np.sign(pred_regime) == np.sign(actual_regime))
        mae_regime = np.mean(np.abs(pred_regime - actual_regime))
        
        print(f"  {regime:8s} ({mask.sum():4d} jours):")
        print(f"    Sharpe    : {sharpe_regime:7.4f}")
        print(f"    Hit Rate  : {hit_rate_regime:7.2%}")
        print(f"    MAE       : {mae_regime:.6f}")
        print()
    
    # 8. Sauvegarder le mod√®le et le preprocessor
    print("=" * 80)
    print("8. SAUVEGARDE DU MOD√àLE")
    print("=" * 80)
    
    # Sauvegarder le mod√®le
    with open(CONFIG['model_path'], 'wb') as f:
        pickle.dump(model, f)
    
    # Sauvegarder le preprocessor
    with open(CONFIG['preprocessor_path'], 'wb') as f:
        pickle.dump(preprocessor, f)
    
    print(f"‚úì Mod√®le sauvegard√©: {CONFIG['model_path']}")
    print(f"‚úì Preprocessor sauvegard√©: {CONFIG['preprocessor_path']}")
    
    # V√©rifier la taille des fichiers
    model_size = Path(CONFIG['model_path']).stat().st_size / (1024 * 1024)
    prep_size = Path(CONFIG['preprocessor_path']).stat().st_size / (1024 * 1024)
    
    print(f"\nTailles des fichiers:")
    print(f"  - Mod√®le: {model_size:.2f} MB")
    print(f"  - Preprocessor: {prep_size:.2f} MB")
    print(f"  - Total: {model_size + prep_size:.2f} MB")
    
    if model_size + prep_size > 100:
        print("\n‚ö†Ô∏è  Warning: Taille totale > 100MB. Optimisation recommand√©e.")
    else:
        print("\n‚úÖ Taille OK pour Kaggle (<100MB)")
    
    # 9. R√©sum√© final
    print("\n" + "=" * 80)
    print("R√âSUM√â FINAL")
    print("=" * 80)
    
    print(f"\nüìä M√©triques cl√©s (Validation):")
    print(f"  - Sharpe Ratio  : {val_metrics['Sharpe']:.4f}")
    print(f"  - Hit Rate      : {val_metrics['Hit_Rate']:.2%}")
    print(f"  - Correlation   : {val_metrics['Correlation']:.4f}")
    print(f"  - Max Drawdown  : {val_metrics['Max_Drawdown']:.2%}")
    print(f"  - MAE           : {val_metrics['MAE']:.6f}")
    
    print(f"\n‚úì Features utilis√©es: {summary['total_features']}")
    print(f"‚úì Lagged features: {summary['lagged_features']}")
    print(f"‚úì Created features: {summary['created_features']}")
    
    print("\n" + "=" * 80)
    print("TRAINING TERMIN√â AVEC SUCC√àS")
    print("=" * 80)
    
    # Recommandations
    print("\nüí° PROCHAINES √âTAPES:")
    
    if sharpe_val > 0.7:
        print("  1. ‚úÖ Tester localement: python test_local.py --mode full")
        print("  2. ‚úÖ Analyser les r√©sultats: python analyze_results.py")
        print("  3. üöÄ SOUMETTRE √Ä KAGGLE !")
    elif sharpe_val > 0.5:
        print("  1. ‚úÖ Tester localement: python test_local.py --mode full")
        print("  2. üìä Analyser les r√©sultats: python analyze_results.py")
        print("  3. ‚ö° Optimiser les hyperparam√®tres (optionnel)")
        print("  4. üöÄ Soumettre √† Kaggle")
    else:
        print("  1. üìä Analyser les r√©sultats: python analyze_results.py")
        print("  2. üîß V√©rifier les features (lagged features utilis√©es ?)")
        print("  3. ‚ö° Optimiser les hyperparam√®tres")
        print("  4. üß™ Re-tester et re-entra√Æner")
    
    return model, preprocessor, val_metrics

# ==============================================================================
# 4. TEST SUR LE FICHIER TEST
# ==============================================================================

def test_on_test_file(model, preprocessor):
    """Tester le mod√®le sur test.csv"""
    
    print("\n" + "=" * 80)
    print("TEST SUR test.csv")
    print("=" * 80)
    
    if not Path('test.csv').exists():
        print("‚ö†Ô∏è  test.csv not found. Skipping test predictions.")
        return None
    
    test = pd.read_csv('test.csv')
    print(f"‚úì Test shape: {test.shape}")
    
    # Preprocessing
    print("\n‚è≥ Preprocessing test data...")
    X_test = preprocessor.transform(test)
    print(f"‚úì Test features shape: {X_test.shape}")
    
    # Pr√©dictions
    print("\n‚è≥ Generating predictions...")
    predictions = model.predict(X_test)
    
    print("\nüìä Pr√©dictions:")
    print(f"  - Nombre: {len(predictions)}")
    print(f"  - Min: {predictions.min():.6f}")
    print(f"  - Max: {predictions.max():.6f}")
    print(f"  - Mean: {predictions.mean():.6f}")
    print(f"  - Std: {predictions.std():.6f}")
    
    # V√©rifier les valeurs anormales
    nan_count = np.isnan(predictions).sum()
    inf_count = np.isinf(predictions).sum()
    
    if nan_count > 0:
        print(f"\n‚ö†Ô∏è  Warning: {nan_count} NaN predictions detected")
    if inf_count > 0:
        print(f"\n‚ö†Ô∏è  Warning: {inf_count} Inf predictions detected")
    
    if nan_count == 0 and inf_count == 0:
        print("\n‚úÖ Toutes les pr√©dictions sont valides")
    
    # Cr√©er un DataFrame de r√©sultats
    results = pd.DataFrame({
        'date_id': test['date_id'],
        'prediction': predictions
    })
    
    print("\nüìã Premi√®res pr√©dictions:")
    print(results.head(10).to_string(index=False))
    
    # Sauvegarder les pr√©dictions
    results.to_csv('test_predictions.csv', index=False)
    print(f"\n‚úì Pr√©dictions sauvegard√©es: test_predictions.csv")
    
    return results

# ==============================================================================
# MAIN
# ==============================================================================

if __name__ == '__main__':
    
    import time
    start_time = time.time()
    
    try:
        # Entra√Æner le mod√®le
        model, preprocessor, metrics = train_baseline_model()
        
        # Tester sur test.csv
        results = test_on_test_file(model, preprocessor)
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "=" * 80)
        print("‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS")
        print("=" * 80)
        print(f"‚è±Ô∏è  Temps total: {elapsed_time/60:.1f} minutes")
        print(f"üéØ Sharpe Ratio: {metrics['Sharpe']:.4f}")
        print(f"üìÅ Fichiers g√©n√©r√©s:")
        print(f"   - {CONFIG['model_path']}")
        print(f"   - {CONFIG['preprocessor_path']}")
        print(f"   - {CONFIG['feature_importance_path']}")
        if results is not None:
            print(f"   - test_predictions.csv")
        
    except Exception as e:
        print("\n" + "=" * 80)
        print("‚ùå ERREUR LORS DE L'EX√âCUTION")
        print("=" * 80)
        print(f"Erreur: {e}")
        import traceback
        traceback.print_exc()
        
        print("\nüí° Suggestions:")
        print("  1. V√©rifiez que train.csv existe")
        print("  2. V√©rifiez que preprocessor_v2.py est accessible")
        print("  3. V√©rifiez les packages install√©s (xgboost, pandas, etc.)")